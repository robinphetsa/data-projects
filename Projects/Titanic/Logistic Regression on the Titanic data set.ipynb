{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've explored the Titanic data set by looking at a variety of features. We learned that gender and age played the biggest influences on the probability of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, we must decide what we want out of the data set before we begin (the whole supervised learning thing right?). In this case, we want to predict whether or not a passenger will survive the sinking of the Titanic or not based on certain features like 'Age', 'Sex', 'Class', etc. So we know this will be a classification problem. Of course that leaves a few possible algorithms such as Decision trees and ensemble methods but I wanted to first start off with Logistic Regression. Despite it's name, it is actually a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Function is defined as follows \\begin{equation*}\n",
    "P(t)= \\frac{1}{1+e^{-t} }\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take any input from 0 to infinity and return values ranging between 0 and 1. To see why, lets look at the parameter 't'. If we make t small, the denominator e^(-t) will be so large that the function P will be close to 0. Likewise, if we make t large, the denominator will be so small that p will be closer to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can substitute 't' with a linear combination of our features. Therefore we would have: \\begin{equation*} P(t)= \\frac{1}{1+e^{- (  \\beta{0}+\\beta{1}x) } } \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a linear combination of our predictors much like we would with linear regression BUT our model instead will produce values between 1 and 0 unlike traditional linear regression. This is perfect for our classification problem. So lets get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
